{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a87424a",
   "metadata": {},
   "source": [
    "## Reference Implementation\n",
    "\n",
    "### E2E Architecture\n",
    "\n",
    "![Use_case_flow](assets/e2e-flow-orig.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1abd81da",
   "metadata": {},
   "source": [
    "### Solution setup\n",
    "Use the following cell to change to the correct kernel. Then check that you are in the `stock` kernel. If not, navigate to `Kernel > Change kernel > Python [conda env:stock]`. Note that the cell will remain with * but you can continue running the following cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235047f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "Jupyter.notebook.session.restart({kernel_name: 'conda-env-cust_seg_stock-py'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca24117",
   "metadata": {},
   "source": [
    "### Reference Implementation\n",
    "\n",
    "#### Model Building Process\n",
    "\n",
    "This customer segmentation approach uses KMeans and DBSCAN from scikit learn library to train an AI model and generate cluster labels for the passed in data.  This process is captured within the `hyperparameter_cluster_analysis.py` script. This script *reads and preprocesses the data*, and *performs hyperparameter cluster analysis on either KMeans or DBSCAN*, while also reporting on the execution time for preprocessing and hyperparameter cluster analysis steps(we will use this information later when we are optimizing the implementation for Intel® architecture).  Furthermore, this script can also save each of the intermediate models/cluster labels for an in-depth analysis of the quality of fit.  \n",
    "\n",
    "The script takes the following arguments:\n",
    "\n",
    "```shell\n",
    "usage: hyperparameter_cluster_analysis.py [-h] [-l LOGFILE] [-i] [--use_small_features] [-r REPEATS] [-a {kmeans,dbscan}]\n",
    "                         [--save_model_dir SAVE_MODEL_DIR]\n",
    "\n",
    "optional arguments:\n",
    "  -h, --help            show this help message and exit\n",
    "  -l LOGFILE, --logfile LOGFILE\n",
    "                        log file to output benchmarking results to\n",
    "  -i, --intel           use intel technologies where available\n",
    "  --use_small_features  use 3 features instead of 21\n",
    "  -r REPEATS, --repeats REPEATS\n",
    "                        number of times to clone the data\n",
    "  -a {kmeans,dbscan}, --algo {kmeans,dbscan}\n",
    "                        clustering algorithm to use\n",
    "  --save_model_dir SAVE_MODEL_DIR\n",
    "                        directory to save ALL models if desired\n",
    "```\n",
    "\n",
    "As an example of using this, we can run the following commands:\n",
    "\n",
    "```shell\n",
    "conda activate cust_seg_stock\n",
    "cd src \n",
    "python hyperparameter_cluster_analysis.py --logfile logs/stock.log --algo kmeans --save_model_dir saved_models\n",
    "```\n",
    "\n",
    "This will perform hyperparameter cluster analysis using KMeans/DBSCAN for the provided data, saving the data to the `saved_models` directory and providing performance logs on the algorithm to the `logs/stock.log` file.  More generally, this script serves to create KMeans/DBSCAN models on 21 features, scanning across various hyperparameters (such as cluster size for KMeans and min_samples for DBSCAN) for each of the models and saving a model at EACH hyperparameter setting to the provided `--save_model_dir` directory.  These saved models can be further analyzed as described below in [Running Cluster Analysis/Predictions](#running-cluster-analysis-predictions).\n",
    "\n",
    "In a realistic pipeline, this process would follow the above diagram, adding either a human in the loop to determine the quality of the clustering solution at each hyperparameter setting, or by adding heuristic measure to quantify cluster quality.  In this situation, we do not implement a clustering quality and instead save the trained models/predictions in the `--save_model_dir` directory at each hyperparameter setting for future analysis and cluster comparisons.\n",
    "\n",
    "As an example of a possible clustering metric, Silhouette analysis is often used for KMeans to help select the number of clusters.  See [here](https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html) for further implementation details.  For example, this can also be used in the above script by adding a rough heuristic that only saves models above a certain heuristic score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c67b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd src && python hyperparameter_cluster_analysis.py --logfile logs/stock.log --algo kmeans --save_model_dir saved_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb52650",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd src && cat logs/stock.log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126ab138",
   "metadata": {},
   "source": [
    "#### Running Cluster Analysis/Predictions\n",
    "\n",
    "The above script will train and save models at different hyperparameter configurations for KMeans or DBSCAN.  In addition to saving the models to `save_model_dir`, the script will also save the following files for each hyper parameter configuration:\n",
    "\n",
    "1. `save_model_dir/data.csv` - preprocessed data file \n",
    "2. `save_model_dir/{algo}/model_{hyperparameters}.pkl` - trained model file \n",
    "3. `save_model_dir/{algo}/pred_{hyperparameters}.txt` - cluster labels for each datapoint in the data file\n",
    "\n",
    "These files can be used to analyze each of the clustering solutions generated from the hyperparameter cluster analysis.  An example snippet of how you can load the saved model files and the predictions for further analysis is:\n",
    "\n",
    "```python\n",
    "import joblib\n",
    "import pandas as pd\n",
    "model = joblib.load(\"saved_models/kmeans/model_{hyperparameters}.pkl\")\n",
    "data = pd.read_csv(\"saved_models/kmeans/data.csv\")\n",
    "cluster_labels = pd.read_csv(\"saved_models/kmeans/preds_{hyperparameters}.txt\", headers=None)\n",
    "```\n",
    "\n",
    "For KMeans, the saved model can be loaded using the `joblib` module and used to predict the cluster label of a new data point.  As an example, this may look like:\n",
    "\n",
    "```python\n",
    "import joblib\n",
    "kmeans_model = joblib.load(\"saved_models/kmeans/model_{hyperparameters}.pkl\")\n",
    "kmeans_model.predict(new_X)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15089fb",
   "metadata": {},
   "source": [
    "### Optimized E2E Architecture with Intel® oneAPI Components\n",
    "\n",
    "![Use_case_flow](assets/e2e-flow-optimized.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c88f30f",
   "metadata": {},
   "source": [
    "### Solution setup\n",
    "Use the following cell to change to the correct kernel. Then check that you are in the `intel` kernel. If not, navigate to `Kernel > Change kernel > Python [conda env:intel]`. Note that the cell will remain with * but you can continue running the following cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3e5dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "Jupyter.notebook.session.restart({kernel_name: 'conda-env-cust_seg_intel-py'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b0379d",
   "metadata": {},
   "source": [
    "#### Model Building Process with Intel® Optimizations\n",
    "\n",
    "The above code is pre-built into the `hyperparameter_cluster_analysis.py` script by adding the `--intel` flag when running the hyperparameter cluster analysis.  The same training process can be run, optimized with Intel® oneAPI as follows:\n",
    "\n",
    "```shell\n",
    "conda activate cust_seg_intel\n",
    "python hyperparameter_cluster_analysis.py --logfile logs/intel.log --algo kmeans --save_model_dir saved_models --intel\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7843c4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd src && python hyperparameter_cluster_analysis.py --logfile logs/intel.log --algo kmeans --save_model_dir saved_models --intel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981d9c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd src && cat logs/intel.log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ff4fad",
   "metadata": {},
   "source": [
    "## Performance Observations\n",
    "\n",
    "For demonstrational purposes of the scaling of Intel® oneAPI Extension for SciKit, we benchmark a **hyperparameter cluster analysis** under the following data-augmentation transformations:\n",
    "\n",
    "1. Using 21 features\n",
    "2. Replicating and jittering the data with noise to have up to 400k rows (depending on algorithm)\n",
    "   1. KMeans - 40k, 400k samples\n",
    "   2. DBSCAN - 40k, 60k samples\n",
    "\n",
    "We summarize the benchmarking results comparing the Intel® technologies vs the stock alternative on the following tasks:\n",
    "\n",
    "  1. hyperparameter cluster analysis via KMeans with 21 Features\n",
    "  2. hyperparameter cluster analysis via DBSCAN with 21 Features\n",
    "\n",
    "where hyperparameter cluster analysis in this case measures the **total time to generate cluster solutions for the given data at each point on the hyperparameter grid for a given algorithm and data size**.  \n",
    "\n",
    "The hyperparameters for each algorithm include:\n",
    "\n",
    "### KMeans\n",
    "|**n_clusters**  | **tol** |                             \n",
    "| :---: | :---: |\n",
    "|2, 3, 4, 5, 10, 15, 20, 25, 30 | 1e-3, 1e-4, 1e-5 |\n",
    "\n",
    "### DBSCAN\n",
    "|**min_samples**  | **eps** |                              \n",
    "| :---: | :---: |\n",
    "|10, 50, 100             | 0.3, 0.5, 0.7 |\n",
    "\n",
    "Noise is added to ensure that no two rows are exactly the same after replication.  DBSCAN testing is limited to 60k samples for because of memory constraints on the machines used for testing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e20f20f",
   "metadata": {},
   "source": [
    "We start by removing any previous logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8a8629",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf logs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c855486",
   "metadata": {},
   "source": [
    "We then run scipts to run the benchmarks. We start by benchmarking the stock solution, we need to first change the kernel to the stock environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a1cb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "Jupyter.notebook.session.restart({kernel_name: 'conda-env-cust_seg_stock-py'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f43bbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd src && bash run_exp_stock.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e46b101",
   "metadata": {},
   "source": [
    "Our run_benchmark function has two parameters, intel toggles to use the intel optomized libraries in the benchmark, additionaly we have a parameter called long_test which can toggle the test above sample sizes mentioned above or a smaller sample size which can run the test benchmark much faster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524b55f5",
   "metadata": {},
   "source": [
    "We will now change to our intel environment and use the intel paramerter to run our benchmark with intel optomizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3166a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "Jupyter.notebook.session.restart({kernel_name: 'conda-env-cust_seg_intel-py'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32fc5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd src && bash run_exp_intel.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8011255",
   "metadata": {},
   "source": [
    "Performance logs have been created and we can use these logs to create graphs comparing performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c0c8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "Jupyter.notebook.session.restart({kernel_name: 'conda-env-cust_seg_stock-py'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5c8dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(os.path.join(os.getenv('workdir'),'src'))\n",
    "from notebooks.utils import benchmarking_utils\n",
    "\n",
    "benchmarking_utils.print_kmeans_plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd5ad5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(os.path.join(os.getenv('workdir'),'src'))\n",
    "from notebooks.utils import benchmarking_utils\n",
    "\n",
    "benchmarking_utils.print_dbscan_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60a4ed9",
   "metadata": {},
   "source": [
    "### Notes\n",
    "***Please see this data set's applicable license for terms and conditions. Intel® does not own the rights to this data set and does not confer any rights to it.***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dce63f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:stock]",
   "language": "python",
   "name": "conda-env-stock-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "a548e12c1901afe13690b9e63a848eb372533911e9ee0d64872d6924ff4c06c5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
